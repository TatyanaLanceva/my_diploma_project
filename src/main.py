import pandas as pd
from pathlib import Path
from sklearn.metrics import accuracy_score, f1_score
import joblib
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from torch.utils.data import Dataset
import numpy as np
import json
from datetime import datetime
from typing import List, Dict, Tuple
import logging
import random
import re

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# === –ù–ê–°–¢–†–û–ô–ö–ò ===
LABELING_FILE_XLSX = "data/operations_for_labeling.xlsx"
MEMORY_FILE = "labeled_memory.pkl"
CORRECTIONS_FILE = "corrections_buffer.json"
MODEL_NAME = "DeepPavlov/rubert-base-cased"
OUTPUT_MODEL_DIR = Path("ruroberta_transaction_classifier")
EXACT_MATCHES_FILE = "exact_matches.json"
REPORT_FILE = "project_report.md"

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
CORRECTION_THRESHOLD = 20
REPLAY_BUFFER_SIZE = 100
INCREMENTAL_LEARNING_RATE = 2e-5

# –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–æ–∫
OUTPUT_MODEL_DIR.mkdir(exist_ok=True, parents=True)

# === –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò ===
def preprocess_text(text: str) -> str:
    if pd.isna(text):
        return ""
    text = str(text).lower().strip()
    text = re.sub(r'\s+', ' ', text)
    return text

def load_initial_data() -> pd.DataFrame:
    try:
        if Path(LABELING_FILE_XLSX).exists():
            logger.info(f"üìä –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ {LABELING_FILE_XLSX}")
            return pd.read_excel(LABELING_FILE_XLSX)
        else:
            logger.error(f"‚ùå –§–∞–π–ª —Å —Ä–∞–∑–º–µ—Ç–∫–æ–π –Ω–µ –Ω–∞–π–¥–µ–Ω: {LABELING_FILE_XLSX}")
            return None
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞: {e}")
        return None

def load_memory():
    if Path(MEMORY_FILE).exists():
        try:
            memory = joblib.load(MEMORY_FILE)
            logger.info(f"üß† –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(memory)} –∑–∞–ø–∏—Å–µ–π –∏–∑ –ø–∞–º—è—Ç–∏")
            return memory
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø–∞–º—è—Ç–∏: {e}")
    
    initial_data = load_initial_data()
    if initial_data is not None:
        memory = initial_data.to_dict('records')
        save_memory(memory)
        return memory
    
    return []

def save_memory(memory):
    try:
        joblib.dump(memory, MEMORY_FILE)
        logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(memory)} –∑–∞–ø–∏—Å–µ–π –≤ –ø–∞–º—è—Ç–∏")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–∞–º—è—Ç–∏: {e}")

# === –ö–õ–ê–°–°–´ –°–ò–°–¢–ï–ú–´ ===
class TransactionDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]
        if pd.isna(text) or text.strip() == "":
            text = "Unknown transaction"
        encoding = self.tokenizer(
            text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt'
        )
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

class CorrectionBuffer:
    def __init__(self, corrections_file: str = CORRECTIONS_FILE):
        self.corrections_file = corrections_file
        self.corrections = self._load_corrections()
        
    def _load_corrections(self) -> List[Dict]:
        if Path(self.corrections_file).exists():
            try:
                with open(self.corrections_file, 'r', encoding='utf-8') as f:
                    corrections = json.load(f)
                logger.info(f"üîÑ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(corrections)} –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π –∏–∑ –±—É—Ñ–µ—Ä–∞")
                return corrections
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π: {e}. –ë—É—Ñ–µ—Ä –±—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω –∑–∞–Ω–æ–≤–æ.")
        return []
    
    def _save_corrections(self):
        try:
            with open(self.corrections_file, 'w', encoding='utf-8') as f:
                json.dump(self.corrections, f, ensure_ascii=False, indent=2, default=str)
            logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(self.corrections)} –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π: {e}")
    
    def add_correction(self, transaction_text: str, predicted_category: str, correct_category: str, 
                       sber_category: str, operation_type: str, confidence: float = 0.0) -> bool:
        correction = {
            'transaction_text': transaction_text,
            'predicted_category': predicted_category,
            'correct_category': correct_category,
            'sber_category': sber_category,
            'operation_type': operation_type,
            'confidence': confidence,
            'timestamp': datetime.now().isoformat(),
            'processed': False
        }
        self.corrections.append(correction)
        self._save_corrections()
        unprocessed_count = len([c for c in self.corrections if not c['processed']])
        logger.info(f"‚ûï –î–æ–±–∞–≤–ª–µ–Ω–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ. –ù–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö: {unprocessed_count}/{CORRECTION_THRESHOLD}")
        return unprocessed_count >= CORRECTION_THRESHOLD
    
    def get_unprocessed_corrections(self) -> List[Dict]:
        return [c for c in self.corrections if not c['processed']]
    
    def mark_as_processed(self):
        for correction in self.corrections:
            if not correction['processed']:
                correction['processed'] = True
        self._save_corrections()
        logger.info("‚úÖ –í—Å–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ—Ç–º–µ—á–µ–Ω—ã –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ")
    
    def get_stats(self) -> Dict:
        if not self.corrections:
            return {'total': 0, 'unprocessed': 0, 'categories': {}}
        unprocessed = self.get_unprocessed_corrections()
        category_stats = {}
        for correction in self.corrections:
            cat = correction['correct_category']
            if cat not in category_stats:
                category_stats[cat] = {'total': 0, 'unprocessed': 0}
            category_stats[cat]['total'] += 1
            if not correction['processed']:
                category_stats[cat]['unprocessed'] += 1
        return {
            'total': len(self.corrections),
            'unprocessed': len(unprocessed),
            'categories': category_stats
        }

class ExactMatchManager:
    def __init__(self, exact_matches_file: str = EXACT_MATCHES_FILE):
        self.exact_matches_file = exact_matches_file
        self.exact_matches = self._load_exact_matches()
    
    def _load_exact_matches(self) -> Dict[str, str]:
        if Path(self.exact_matches_file).exists():
            try:
                with open(self.exact_matches_file, 'r', encoding='utf-8') as f:
                    matches = json.load(f)
                logger.info(f"üéØ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(matches)} —Ç–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {e}. –ë—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π —Ñ–∞–π–ª.")
                matches = {}
            return matches
        return {}
    
    def _save_exact_matches(self):
        try:
            with open(self.exact_matches_file, 'w', encoding='utf-8') as f:
                json.dump(self.exact_matches, f, ensure_ascii=False, indent=2)
            logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(self.exact_matches)} —Ç–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {e}")
    
    def add_exact_match(self, combined_text: str, category: str):
        normalized_text = combined_text.lower().strip()
        self.exact_matches[normalized_text] = category
        self._save_exact_matches()
    
    def get_exact_match(self, combined_text: str) -> str:
        normalized_text = combined_text.lower().strip()
        return self.exact_matches.get(normalized_text, None)

class CustomTrainer(Trainer):
    def __init__(self, *args, class_weights=None, **kwargs):
        super().__init__(*args, **kwargs)
        self.class_weights = class_weights

    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        logits = outputs.get("logits")
        
        if self.class_weights is not None:
            loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights.to(logits.device))
            loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))
        else:
            loss_fct = torch.nn.CrossEntropyLoss()
            loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))
            
        return (loss, outputs) if return_outputs else loss

class IncrementalTransactionClassifier:
    def __init__(self, model_dir: str = str(OUTPUT_MODEL_DIR)):
        self.model_dir = Path(model_dir)
        self.correction_buffer = CorrectionBuffer()
        self.exact_match_manager = ExactMatchManager()
        self.model = None
        self.tokenizer = None
        self.label_encoder = None
        self._load_or_create_model()
    
    def _load_or_create_model(self):
        try:
            if (self.model_dir / "config.json").exists() and (self.model_dir / "label_encoder.pkl").exists():
                logger.info(f"üìÇ –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –º–æ–¥–µ–ª—å –∏–∑ {self.model_dir}")
                self.tokenizer = AutoTokenizer.from_pretrained(self.model_dir)
                self.model = AutoModelForSequenceClassification.from_pretrained(self.model_dir)
                self.label_encoder = joblib.load(self.model_dir / "label_encoder.pkl")
                logger.info("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            else:
                logger.info("üÜï –§–∞–π–ª—ã –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å.")
                self._create_new_model()
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
            logger.info("üîÑ –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å")
            self._create_new_model()
            
    def _create_new_model(self):
        logger.info("üöÄ –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏...")
        df_memory = load_initial_data()
        if df_memory is None or df_memory.empty:
            logger.error("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏")
            return
        df_memory = self._clean_data(df_memory)
        if df_memory.empty:
            logger.error("‚ùå –ù–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            return
        self.label_encoder = LabelEncoder()
        df_memory['label'] = self.label_encoder.fit_transform(df_memory['custom_category'])
        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
        self.model = AutoModelForSequenceClassification.from_pretrained(
            MODEL_NAME, num_labels=len(self.label_encoder.classes_)
        )
        self._train_model(df_memory)
        self._save_model()
    
    def _clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        df = df.copy()
        df.dropna(subset=['description', 'custom_category'], inplace=True)
        df = df[df['custom_category'].str.strip() != '']
        df['sber_category'] = df['sber_category'].astype(str).fillna('–ù–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∏').str.strip()
        df['operation_type'] = df['operation_type'].astype(str).fillna('–ù–µ—Ç —Ç–∏–ø–∞').str.strip()
        df['description'] = df['description'].astype(str).str.strip()
        df['custom_category'] = df['custom_category'].astype(str).str.strip()
        df = df[df['description'] != '']
        df = df[df['description'] != 'nan']
        df['combined_text'] = (
            df['description'].apply(preprocess_text) +
            ' | SBER: ' + df['sber_category'].apply(preprocess_text) +
            ' | –¢–ò–ü: ' + df['operation_type'].apply(preprocess_text)
        )
        class_counts = df['custom_category'].value_counts()
        classes_to_keep = class_counts[class_counts >= 2].index
        df = df[df['custom_category'].isin(classes_to_keep)]
        return df.reset_index(drop=True)
    
    def _train_model(self, df: pd.DataFrame, is_incremental: bool = False):
        X = df['combined_text'].tolist()
        y = df['label'].values
        if len(y) < 2:
            logger.warning("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.")
            return
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        train_dataset = TransactionDataset(X_train, y_train, self.tokenizer)
        val_dataset = TransactionDataset(X_val, y_val, self.tokenizer)
        class_counts = pd.Series(y_train).value_counts()
        total_samples = len(y_train)
        class_weights = total_samples / (len(class_counts) * class_counts)
        weights_tensor = torch.tensor(class_weights.loc[range(len(self.label_encoder.classes_))].values, dtype=torch.float)
        num_epochs = 20 if not is_incremental else 2
        learning_rate = INCREMENTAL_LEARNING_RATE if is_incremental else 3e-5
        training_args = TrainingArguments(
            output_dir=self.model_dir / "temp",
            num_train_epochs=num_epochs,
            per_device_train_batch_size=8,
            per_device_eval_batch_size=8,
            eval_strategy="epoch",
            save_strategy="no",
            logging_steps=20,
            dataloader_num_workers=0,
            learning_rate=learning_rate,
            warmup_ratio=0.1,
            load_best_model_at_end=False,
            metric_for_best_model='f1'
        )
        def compute_metrics(pred):
            labels = pred.label_ids
            preds = pred.predictions.argmax(-1)
            return {'accuracy': accuracy_score(labels, preds), 'f1': f1_score(labels, preds, average='weighted')}
        trainer = CustomTrainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            compute_metrics=compute_metrics,
            class_weights=weights_tensor
        )
        if torch.cuda.is_available():
            self.model.to('cuda')
        logger.info(f"‚è≥ {'–ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ' if is_incremental else '–ü–æ–ª–Ω–æ–µ'} –æ–±—É—á–µ–Ω–∏–µ...")
        trainer.train()
        eval_result = trainer.evaluate()
        logger.info(f"üìà Accuracy: {eval_result['eval_accuracy']:.4f}, F1: {eval_result['eval_f1']:.4f}")
    
    def _save_model(self):
        try:
            self.model.save_pretrained(self.model_dir)
            self.tokenizer.save_pretrained(self.model_dir)
            joblib.dump(self.label_encoder, self.model_dir / "label_encoder.pkl")
            logger.info(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {self.model_dir}")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")
    
    def predict_and_get_categories(self, transaction_text: str, sber_category: str = "", operation_type: str = "") -> Tuple[str, float, List[str]]:
        combined_text = (
            preprocess_text(transaction_text) +
            ' | SBER: ' + preprocess_text(sber_category) +
            ' | –¢–ò–ü: ' + preprocess_text(operation_type)
        )
        exact_match = self.exact_match_manager.get_exact_match(combined_text)
        if exact_match:
            logger.info(f"üéØ –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –Ω–∞–π–¥–µ–Ω–æ: {exact_match}")
            return exact_match, 1.0, list(self.label_encoder.classes_)
        if not self.model or not self.tokenizer or not self.label_encoder:
            logger.error("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            return "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ", 0.0, []
        try:
            encoding = self.tokenizer(
                combined_text, truncation=True, padding=True, max_length=128, return_tensors='pt'
            )
            with torch.no_grad():
                outputs = self.model(**encoding)
                probabilities = torch.softmax(outputs.logits, dim=-1)
                confidence, predicted_class = torch.max(probabilities, dim=1)
            predicted_category = self.label_encoder.inverse_transform([predicted_class.item()])[0]
            confidence_score = confidence.item()
            return predicted_category, confidence_score, list(self.label_encoder.classes_)
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
            return "–û—à–∏–±–∫–∞", 0.0, []
    
    def predict_batch(self, texts: list) -> list:
        """–ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ –≤ –æ–¥–Ω–æ–º –±–∞—Ç—á–µ."""
        if not self.model or not self.tokenizer or not self.label_encoder:
            return ["–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"] * len(texts)
        try:
            encoding = self.tokenizer(
                texts, truncation=True, padding=True, max_length=128, return_tensors='pt'
            )
            device = self.model.device
            with torch.no_grad():
                outputs = self.model(**{k: v.to(device) for k, v in encoding.items()})
                predictions = torch.argmax(outputs.logits, dim=-1).cpu().numpy()
            return self.label_encoder.inverse_transform(predictions).tolist()
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
            return ["–û—à–∏–±–∫–∞"] * len(texts)
    
    def add_correction(self, transaction_text: str, predicted_category: str, correct_category: str, 
                       sber_category: str, operation_type: str, confidence: float = 0.0) -> Dict[str, str]:
        combined_text = (
            preprocess_text(transaction_text) +
            ' | SBER: ' + preprocess_text(sber_category) +
            ' | –¢–ò–ü: ' + preprocess_text(operation_type)
        )
        self.exact_match_manager.add_exact_match(combined_text, correct_category)
        should_retrain = self.correction_buffer.add_correction(
            transaction_text, predicted_category, correct_category, sber_category, operation_type, confidence
        )
        result = {'status': 'added', 'message': f'–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–æ–±–∞–≤–ª–µ–Ω–æ. –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ.'}
        if should_retrain:
            retrain_result = self.incremental_retrain()
            result['retrain_status'] = retrain_result['status']
            result['message'] += f" {retrain_result['message']}"
        return result
    
    def incremental_retrain(self) -> Dict[str, str]:
        try:
            corrections = self.correction_buffer.get_unprocessed_corrections()
            if not corrections:
                return {'status': 'no_data', 'message': '–ù–µ—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è'}
            logger.info(f"üîÑ –ù–∞—á–∏–Ω–∞–µ–º –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ {len(corrections)} –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è—Ö")
            correction_df = pd.DataFrame(corrections)
            correction_df = self._clean_data(correction_df.rename(columns={'transaction_text': 'description', 'correct_category': 'custom_category'}))
            old_data = self._get_replay_buffer(len(corrections))
            all_data = old_data + correction_df.to_dict('records')
            train_df = pd.DataFrame(all_data)
            existing_categories = set(self.label_encoder.classes_)
            new_categories = set(train_df['custom_category']) - existing_categories
            if new_categories:
                logger.info(f"üÜï –ù–∞–π–¥–µ–Ω—ã –Ω–æ–≤—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {new_categories}")
                all_known_categories = list(existing_categories) + list(new_categories)
                self.label_encoder = LabelEncoder()
                self.label_encoder.fit(all_known_categories)
                self.model = AutoModelForSequenceClassification.from_pretrained(
                    MODEL_NAME, num_labels=len(self.label_encoder.classes_)
                )
            train_df['label'] = self.label_encoder.transform(train_df['custom_category'])
            self._train_model(train_df, is_incremental=True)
            self._save_model()
            self.correction_buffer.mark_as_processed()
            return {
                'status': 'success', 
                'message': f'–ú–æ–¥–µ–ª—å –¥–æ–æ–±—É—á–µ–Ω–∞ –Ω–∞ {len(corrections)} –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è—Ö + {len(old_data)} —Å—Ç–∞—Ä—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö'
            }
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è: {e}")
            return {'status': 'error', 'message': f'–û—à–∏–±–∫–∞: {str(e)}'}
    
    def _get_replay_buffer(self, correction_count: int) -> List[Dict]:
        memory = load_memory()
        if not memory:
            return []
        sample_size = min(REPLAY_BUFFER_SIZE, len(memory), correction_count * 3)
        if sample_size < len(memory):
            memory = random.sample(memory, sample_size)
        logger.info(f"üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º {len(memory)} —Å—Ç–∞—Ä—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è replay buffer")
        return memory
    
    def get_stats(self) -> Dict:
        correction_stats = self.correction_buffer.get_stats()
        exact_matches_count = len(self.exact_match_manager.exact_matches)
        model_info = {
            'model_loaded': self.model is not None,
            'num_classes': len(self.label_encoder.classes_) if self.label_encoder else 0,
            'classes': list(self.label_encoder.classes_) if self.label_encoder else []
        }
        return {
            'corrections': correction_stats,
            'exact_matches': exact_matches_count,
            'model': model_info
        }

# === –§–£–ù–ö–¶–ò–ò –ì–ï–ù–ï–†–ê–¶–ò–ò –û–¢–ß–Å–¢–ê ===
def get_project_stats():
    stats = {}
    try:
        if Path(CORRECTIONS_FILE).exists():
            with open(CORRECTIONS_FILE, 'r', encoding='utf-8') as f:
                corrections = json.load(f)
            df_corrections = pd.DataFrame(corrections)
            stats['total_corrections'] = len(df_corrections)
            stats['unprocessed_corrections'] = len(df_corrections[~df_corrections['processed']])
            if not df_corrections.empty:
                correction_categories = df_corrections['correct_category'].value_counts()
                stats['top_corrected_categories'] = correction_categories.head(5).to_dict()
        else:
            stats.update({'total_corrections': 0, 'unprocessed_corrections': 0, 'top_corrected_categories': {}})
        if Path(EXACT_MATCHES_FILE).exists():
            with open(EXACT_MATCHES_FILE, 'r', encoding='utf-8') as f:
                exact_matches = json.load(f)
            stats['total_exact_matches'] = len(exact_matches)
        else:
            stats['total_exact_matches'] = 0
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±–æ—Ä–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
        stats.update({'total_corrections': 'N/A', 'unprocessed_corrections': 'N/A', 'total_exact_matches': 'N/A'})
    return stats

def generate_markdown_report(report_data):
    report = f"# –û—Ç—á–µ—Ç –ø–æ –ø—Ä–æ–µ–∫—Ç—É ¬´–°–∏—Å—Ç–µ–º–∞ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π¬ª\n"
    report += f"**–î–∞—Ç–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
    report += "---\n\n"
    report += f"## 1. –û–±—â–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ üìä\n"
    report += f"- **–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤:** {report_data.get('num_classes', 'N/A')}\n"
    report += f"- **–í—Å–µ–≥–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π:** {report_data['total_corrections']}\n"
    report += f"- **–ù–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π:** {report_data['unprocessed_corrections']}\n"
    report += f"- **–¢–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π:** {report_data['total_exact_matches']}\n\n"
    if report_data['top_corrected_categories']:
        report += f"### –¢–æ–ø-5 –∫–∞—Ç–µ–≥–æ—Ä–∏–π, —Ç—Ä–µ–±—É—é—â–∏—Ö –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π:\n"
        for category, count in report_data['top_corrected_categories'].items():
            report += f"- **{category}**: {count} —Ä–∞–∑\n"
        report += "\n"
    report += "---\n\n"
    report += f"## 2. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏ üìà\n"
    report += f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–ª—É—á–µ–Ω—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏ –Ω–∞ –≤—Å—ë–º —Ä–∞–∑–º–µ—á–µ–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ.\n"
    accuracy_str = f"{report_data['accuracy']:.4f}" if isinstance(report_data['accuracy'], (int, float)) else str(report_data['accuracy'])
    f1_score_str = f"{report_data['f1_score']:.4f}" if isinstance(report_data['f1_score'], (int, float)) else str(report_data['f1_score'])
    report += f"- **–¢–æ—á–Ω–æ—Å—Ç—å (Accuracy):** {accuracy_str}\n"
    report += f"- **F1-–º–µ—Ä–∞ (F1-score):** {f1_score_str}\n\n"
    report += "---\n\n"
    report += f"## 3. –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫ üîç\n"
    report += f"–ù–∏–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã 5 —Å–ª—É—á–∞–π–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤, –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –º–æ–¥–µ–ª—å –æ—à–∏–±–ª–∞—Å—å.\n"
    if report_data['error_sample']:
        report += "| –û–ø–∏—Å–∞–Ω–∏–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ | –ò—Å—Ç–∏–Ω–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è | –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å—é |\n"
        report += "|---------------------|--------------------|-----------------------|\n"
        for error in report_data['error_sample']:
            desc = str(error['description'])[:50] + '...' if len(str(error['description'])) > 50 else str(error['description'])
            report += f"| {desc} | {error['true']} | {error['predicted']} |\n"
        report += "\n"
    else:
        report += "–û—à–∏–±–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ. –û—Ç–ª–∏—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç!\n\n"
    report += "---\n\n"
    report += "### üí° –í—ã–≤–æ–¥—ã\n"
    report += "–°–∏—Å—Ç–µ–º–∞ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —É—Å–ø–µ—à–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç, —É–ª—É—á—à–∞—è –∫–∞—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å –∫–∞–∂–¥—ã–º –Ω–æ–≤—ã–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º.\n"
    return report

def evaluate_and_report(classifier: IncrementalTransactionClassifier):
    """
    –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ –ø–æ–ª–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á—ë—Ç.
    """
    print("üìã –ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á—ë—Ç–∞...")
    data_path = Path(LABELING_FILE_XLSX)
    if not data_path.exists():
        print(f"‚ùå –§–∞–π–ª —Å —Ä–∞–∑–º–µ—Ç–∫–æ–π '{data_path}' –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return
    try:
        df = pd.read_excel(data_path, engine='openpyxl')
        df = classifier._clean_data(df.copy())
        if df.empty:
            print("‚ö†Ô∏è –ü–æ—Å–ª–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏.")
            return
        y_true = df['custom_category'].astype(str).tolist()
        y_pred = classifier.predict_batch(df['combined_text'].tolist())
        df['predicted_category'] = y_pred
        df['is_correct'] = df['custom_category'] == df['predicted_category']
        df_errors = df[~df['is_correct']].copy()
        accuracy = accuracy_score(y_true, y_pred)
        f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)
        print("\n" + "="*50)
        print(f"üìà –¢–æ—á–Ω–æ—Å—Ç—å (Accuracy): {accuracy:.4f}")
        print(f"üìà F1-–º–µ—Ä–∞ (F1-score): {f1:.4f}")
        print(f"‚ùå –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –æ—à–∏–±–æ–∫: {len(df_errors)}")
        print("="*50)
        report_data = {
            **get_project_stats(),
            'accuracy': accuracy,
            'f1_score': f1,
            'num_classes': len(classifier.label_encoder.classes_),
            'error_sample': df_errors.sample(min(5, len(df_errors)))[['description', 'custom_category', 'predicted_category']].rename(columns={'custom_category': 'true', 'predicted_category': 'predicted'}).to_dict('records')
        }
        report_content = generate_markdown_report(report_data)
        with open(REPORT_FILE, 'w', encoding='utf-8') as f:
            f.write(report_content)
        print(f"\n‚úÖ –û—Ç—á—ë—Ç —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ —Ñ–∞–π–ª: {REPORT_FILE}")
        print("–ü—Ä–æ—Ü–µ—Å—Å –æ—Ü–µ–Ω–∫–∏ –∑–∞–≤–µ—Ä—à—ë–Ω.")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏–∏ –æ—Ü–µ–Ω–∫–∏ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á—ë—Ç–∞: {e}")

# === –û–°–ù–û–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –ü–†–û–ì–†–ê–ú–ú–´ ===
def main():
    print("üöÄ –°–∏—Å—Ç–µ–º–∞ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π")
    print("======================================================================")
    print("–í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã:")
    print("1. –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è —Ä–∞–±–æ—Ç–∞ (–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∏ –¥–æ—Ä–∞–±–æ—Ç–∫–∞)")
    print("2. –ü—Ä–æ–≤–µ–¥–µ–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞")
    
    choice = input("–í–≤–µ–¥–∏—Ç–µ –Ω–æ–º–µ—Ä —Ä–µ–∂–∏–º–∞ (1 –∏–ª–∏ 2): ")
    
    classifier = IncrementalTransactionClassifier()
    
    if choice == '1':
        interactive_mode(classifier)
    elif choice == '2':
        evaluate_and_report(classifier)
    else:
        print("–ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã.")

def interactive_mode(classifier):
    stats = classifier.get_stats()
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∏—Å—Ç–µ–º—ã:")
    print(f" ¬† - –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {stats['model']['model_loaded']}")
    print(f" ¬† - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {stats['model']['num_classes']}")
    print(f" ¬† - –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π –≤ –±—É—Ñ–µ—Ä–µ: {stats['corrections']['total']} (–Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö: {stats['corrections']['unprocessed']})")
    print(f" ¬† - –¢–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {stats['exact_matches']}")
    print("\nüí° –î–ª—è –≤—ã—Ö–æ–¥–∞ –∏–∑ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞ –Ω–∞–∂–º–∏—Ç–µ Ctrl+C.")

    while True:
        try:
            print("\n" + "="*50)
            trans_text = input("–í–≤–µ–¥–∏—Ç–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, '–ü–æ–∫—É–ø–∫–∞ –≤ –ú–∞–≥–Ω–∏—Ç'): ")
            sber_cat = input("–í–≤–µ–¥–∏—Ç–µ –∫–∞—Ç–µ–≥–æ—Ä–∏—é –æ—Ç –°–±–µ—Ä–±–∞–Ω–∫–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, '–ü—Ä–æ–¥—É–∫—Ç—ã –ø–∏—Ç–∞–Ω–∏—è'): ")
            op_type = input("–í–≤–µ–¥–∏—Ç–µ —Ç–∏–ø –æ–ø–µ—Ä–∞—Ü–∏–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, '–†–∞—Å—Ö–æ–¥'): ")
            predicted_cat, confidence, all_categories = classifier.predict_and_get_categories(
                trans_text, sber_cat, op_type
            )
            print(f"\nüîç –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏:")
            print(f" ¬† –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è: {predicted_cat}")
            print(f" ¬† –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.4f}")
            if confidence < 0.8:
                print("\nü§î –ú–æ–¥–µ–ª—å –Ω–µ —É–≤–µ—Ä–µ–Ω–∞. –ü–æ–¥—Å–∫–∞–∂–∏—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é.")
                print("–°–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–π:")
                for i, cat in enumerate(sorted(all_categories)):
                    print(f" ¬† {i+1}: {cat}")
                print(f"\n ¬† 0: '{predicted_cat}' (–ü–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–µ)")
                user_choice = input("–í–≤–µ–¥–∏—Ç–µ –Ω–æ–º–µ—Ä –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–ª–∏ '0' –¥–ª—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è: ")
                if user_choice.isdigit():
                    choice_index = int(user_choice)
                    if 1 <= choice_index <= len(all_categories):
                        correct_category = sorted(all_categories)[choice_index - 1]
                        print(f"‚úÖ –í—ã –≤—ã–±—Ä–∞–ª–∏: {correct_category}")
                        result = classifier.add_correction(
                            transaction_text=trans_text, predicted_category=predicted_cat, correct_category=correct_category,
                            sber_category=sber_cat, operation_type=op_type, confidence=confidence
                        )
                        print(f" ¬† –†–µ–∑—É–ª—å—Ç–∞—Ç: {result['message']}")
                    elif choice_index == 0:
                        print("‚úÖ –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∞.")
                    else:
                        print("‚ö†Ô∏è –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –≤—ã–±–æ—Ä. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.")
                else:
                    print("‚ö†Ô∏è –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –≤–≤–æ–¥. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ —á–∏—Å–ª–æ.")
            else:
                print("\n‚úÖ –ú–æ–¥–µ–ª—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —É–≤–µ—Ä–µ–Ω–∞ –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏. –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.")
        except KeyboardInterrupt:
            print("\n\nüëã –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º –∑–∞–≤–µ—Ä—à–µ–Ω. –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            break
        except Exception as e:
            logger.error(f"‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –Ω–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
            print("\n‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.")

if __name__ == "__main__":
    main()